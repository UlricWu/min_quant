{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T02:49:04.289553662Z",
     "start_time": "2025-12-22T02:49:04.239259593Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "from src.utils.path import PathManager\n",
    "from src.config.app_config import AppConfig\n",
    "from src.observability.instrumentation import Instrumentation\n",
    "from src.pipeline.step import BasePipelineStep\n",
    "from src.pipeline.context import PipelineContext\n",
    "from src.utils.filesystem import FileSystem\n",
    "from src.utils.logger import logs"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:42:54.959031774Z",
     "start_time": "2025-12-22T02:42:54.939237817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg = AppConfig.load()\n",
    "pm = PathManager()\n",
    "inst = Instrumentation()\n",
    "d = '2015-01-01'\n",
    "\n",
    "raw_dir = pm.raw_dir(d)\n",
    "parquet_dir = pm.parquet_dir(d)\n",
    "symbol_dir = pm.symbol_dir(d)\n",
    "normalize_dir = pm.canonical_dir(d)\n",
    "meta_dir = pm.meta_dir(d)\n",
    "\n",
    "FileSystem.ensure_dir(raw_dir)\n",
    "FileSystem.ensure_dir(parquet_dir)\n",
    "FileSystem.ensure_dir(symbol_dir)\n",
    "FileSystem.ensure_dir(normalize_dir)\n",
    "FileSystem.ensure_dir(meta_dir)\n",
    "\n",
    "ctx = PipelineContext(\n",
    "    date=d,\n",
    "    raw_dir=raw_dir,\n",
    "    parquet_dir=parquet_dir,\n",
    "    symbol_dir=symbol_dir,\n",
    "    canonical_dir=normalize_dir,\n",
    "    meta_dir=meta_dir\n",
    ")\n"
   ],
   "id": "4a4f463cf5e4b1b7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:42:55.670629217Z",
     "start_time": "2025-12-22T02:42:55.587970805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CsvConvertStep(BasePipelineStep):\n",
    "    def __init__(self, engine, inst=None):\n",
    "        super().__init__(inst)\n",
    "        self.engine = engine\n",
    "\n",
    "    def run(self, ctx: PipelineContext):\n",
    "        input_dir = ctx.raw_dir\n",
    "        out_dir = ctx.parquet_dir\n",
    "\n",
    "        for zfile in input_dir.glob(\"*.7z\"):\n",
    "            out_files = self._build_out_files(zfile, out_dir)\n",
    "            if self._all_exist(out_files):\n",
    "                print(f\"[CsvConvertStep] skip {zfile.name}\")\n",
    "                continue\n",
    "            print(f\"[CsvConvertStep]  {zfile.name} {out_files}\")\n",
    "            self.engine.convert(zfile, out_files)\n",
    "\n",
    "    def _detect_type(self, filename):\n",
    "        \"\"\"\n",
    "        根据文件名约定识别 file_type：\n",
    "            SH_Stock_OrderTrade.csv.7z → SH_MIXED\n",
    "            SH_Order.csv.7z           → SH_ORDER\n",
    "            SH_Trade.csv.7z           → SH_TRADE\n",
    "            SZ_Order.csv.7z           → SZ_ORDER\n",
    "            SZ_Trade.csv.7z           → SZ_TRADE\n",
    "        \"\"\"\n",
    "        lower = filename.lower()\n",
    "\n",
    "        if lower.startswith(\"sh_stock_ordertrade\"):\n",
    "            return \"SH_MIXED\"\n",
    "\n",
    "        if lower.startswith(\"sh_order\"):\n",
    "            return \"SH_ORDER\"\n",
    "        if lower.startswith(\"sh_trade\"):\n",
    "            return \"SH_TRADE\"\n",
    "\n",
    "        if lower.startswith(\"sz_order\"):\n",
    "            return \"SZ_ORDER\"\n",
    "        if lower.startswith(\"sz_trade\"):\n",
    "            return \"SZ_TRADE\"\n",
    "\n",
    "        raise RuntimeError(f\"无法识别文件类型: {filename}\")\n",
    "\n",
    "    def _build_out_files(self, zfile: Path, parquet_dir: Path) -> dict[str, Path]:\n",
    "        file_type = self._detect_type(zfile.stem)\n",
    "        if file_type == \"SH_MIXED\":\n",
    "            return {\n",
    "                \"sh_order\": parquet_dir / \"sh_order.parquet\",\n",
    "                \"sh_trade\": parquet_dir / \"sh_trade.parquet\",\n",
    "            }\n",
    "\n",
    "        stem = zfile.stem.replace(\".csv\", \"\")\n",
    "        return {\n",
    "            stem.lower(): parquet_dir / f\"{stem.lower()}.parquet\"\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _all_exist(out_files: dict[str, Path]) -> bool:\n",
    "        return all(p.exists() for p in out_files.values())\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from src.engines.extractor_engine import ExtractorEngine\n",
    "\n",
    "\n",
    "class ParquetAppendWriter:\n",
    "    def __init__(self):\n",
    "        self._schemas: dict[Path, pa.Schema] = {}\n",
    "        self._writers: dict[Path, pq.ParquetWriter] = {}\n",
    "\n",
    "    def write_batches(self, path: Path, batches: list[pa.RecordBatch]) -> None:\n",
    "        if not batches:\n",
    "            return\n",
    "\n",
    "        writer = self._writers.get(path)\n",
    "        if writer is None:\n",
    "            # schema 来自第一个 batch\n",
    "            schema = batches[0].schema\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            writer = pq.ParquetWriter(\n",
    "                path,\n",
    "                schema,\n",
    "                compression=\"zstd\",\n",
    "            )\n",
    "            self._writers[path] = writer\n",
    "            self._schemas[path] = schema\n",
    "\n",
    "        table = pa.Table.from_batches(batches, schema=self._schemas[path])\n",
    "        writer.write_table(table)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        for writer in self._writers.values():\n",
    "            writer.close()\n",
    "        self._writers.clear()\n",
    "\n",
    "\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "\n",
    "class ConvertEngine:\n",
    "    ORDER_TYPES = [\"A\", \"D\", \"M\"]\n",
    "    TRADE_TYPE = \"T\"\n",
    "    TICK_COL = \"TickType\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.extractor = ExtractorEngine\n",
    "        self.order_set = pa.array(self.ORDER_TYPES)\n",
    "        self.trade_value = pa.scalar(self.TRADE_TYPE)\n",
    "\n",
    "    def convert(self, zfile: Path, out_files: dict[str, Path]) -> None:\n",
    "        reader = self.extractor.open_reader(zfile)\n",
    "        writer = ParquetAppendWriter()\n",
    "        try:\n",
    "            for batch in reader:\n",
    "                batch = self.extractor.cast_strings(batch)\n",
    "\n",
    "                if len(out_files) == 1:\n",
    "                    # 非拆分\n",
    "                    key = next(iter(out_files))\n",
    "                    writer.write_batches(out_files[key], [batch])\n",
    "                else:\n",
    "                    # 拆分\n",
    "                    for key, sub_batch in self._split(batch, out_files).items():\n",
    "                        if sub_batch.num_rows:\n",
    "                            writer.write_batches(out_files[key], [sub_batch])\n",
    "        finally:\n",
    "            writer.close()\n",
    "\n",
    "    def _split(self, batch: pa.RecordBatch, out_files: dict[str, Path]) -> dict[str, pa.RecordBatch]:\n",
    "        \"\"\"返回 (order_batch, trade_batch)\"\"\"\n",
    "        if self.TICK_COL not in batch.schema.names:\n",
    "            raise ValueError(f\"missing column: {self.TICK_COL}\")\n",
    "\n",
    "        idx = batch.schema.get_field_index(self.TICK_COL)\n",
    "        tick_arr = batch.column(idx)\n",
    "\n",
    "        order_mask = pc.is_in(tick_arr, self.order_set)\n",
    "        trade_mask = pc.equal(tick_arr, self.trade_value)\n",
    "\n",
    "        result = {}\n",
    "        for key in out_files:\n",
    "            if \"order\" in key:\n",
    "                result[key] = batch.filter(order_mask)\n",
    "            elif \"trade\" in key:\n",
    "                result[key] = batch.filter(trade_mask)\n",
    "\n",
    "        return result\n"
   ],
   "id": "e94fbeace54c539a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:42:56.211058686Z",
     "start_time": "2025-12-22T02:42:56.183519577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from src import DateTimeUtils\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "class NormalizeEngine:\n",
    "    \"\"\"\n",
    "    NormalizeEngine（冻结契约版）\n",
    "\n",
    "    - 输入：交易所级 parquet\n",
    "    - 输出：canonical order / trade parquet\n",
    "    - symbol 只是字段，不做拆分\n",
    "    \"\"\"\n",
    "\n",
    "    VALID_EVENTS = {\"ADD\", \"CANCEL\", \"TRADE\"}\n",
    "    batch_size = 1_000_0000\n",
    "\n",
    "    def execute(self, input_file: Path, output_dir: Path) -> None:\n",
    "        exchange, kind = input_file.stem.split(\"_\", 1)\n",
    "        out_path = output_dir / input_file.name\n",
    "\n",
    "        pf = pq.ParquetFile(input_file)\n",
    "        writer = None\n",
    "\n",
    "        for batch in pf.iter_batches(self.batch_size):\n",
    "            table = pa.Table.from_batches([batch])\n",
    "            table = self.filter_a_share_arrow(table)\n",
    "            if table.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            table = parse_events_arrow(\n",
    "                table,\n",
    "                exchange=exchange,\n",
    "                kind=kind,\n",
    "            )\n",
    "\n",
    "            if table.num_rows == 0:\n",
    "                continue\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(out_path, table.schema)\n",
    "\n",
    "            writer.write_table(table)\n",
    "\n",
    "        if writer:\n",
    "            writer.close()\n",
    "\n",
    "    def filter_a_share_arrow(self, table: pa.Table) -> pa.Table:\n",
    "        symbol = pc.cast(table[\"SecurityID\"], pa.string())\n",
    "\n",
    "        # prefixes = [\n",
    "        #     \"600\", \"601\", \"603\", \"605\", \"688\",\n",
    "        #     \"000\", \"001\", \"002\", \"003\", \"300\",\n",
    "        # ]\n",
    "        prefixes = [\n",
    "            \"60\", \"688\",\n",
    "            \"00\", \"300\",\n",
    "        ]\n",
    "\n",
    "        masks = [pc.starts_with(symbol, p) for p in prefixes]\n",
    "\n",
    "        mask = reduce(pc.or_, masks)\n",
    "\n",
    "        return table.filter(mask)\n",
    "\n",
    "\n",
    "class NormalizeStep(BasePipelineStep):\n",
    "    def __init__(self, engine: NormalizeEngine, inst=None):\n",
    "        super().__init__(inst)\n",
    "        self.engine = engine\n",
    "\n",
    "    def run(self, ctx: PipelineContext) -> PipelineContext:\n",
    "        input_dir: Path = ctx.parquet_dir\n",
    "        output_dir: Path = ctx.canonical_dir\n",
    "\n",
    "        for file in list(input_dir.glob(\"*.parquet\")):\n",
    "            filename = file.stem\n",
    "            output_file = output_dir / filename\n",
    "\n",
    "            if output_file.exists():\n",
    "                logs.info(f'')\n",
    "                continue\n",
    "            self.engine.execute(\n",
    "                input_file=input_dir / file,\n",
    "                output_dir=output_dir,\n",
    "            )\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# Internal Event Schema（唯一真相）\n",
    "# =============================================================================\n",
    "# from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "from typing import Literal\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "EventKind = Literal[\"order\", \"trade\"]\n",
    "\n",
    "INTERNAL_SCHEMA = pa.schema(\n",
    "    [('symbol', pa.string()),\n",
    "     (\"ts\", pa.int64()),\n",
    "     (\"event\", pa.string()),\n",
    "     (\"order_id\", pa.int64()),\n",
    "     (\"side\", pa.string()),\n",
    "     (\"price\", pa.float64()),\n",
    "     (\"volume\", pa.int64()),\n",
    "     (\"buy_no\", pa.int64()),\n",
    "     (\"sell_no\", pa.int64()),\n",
    "     ]\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ExchangeDefinition:\n",
    "    symbol_field: str\n",
    "    time_field: str\n",
    "    event_field: str\n",
    "    event_mapping: Dict\n",
    "    price_field: str\n",
    "    volume_field: str\n",
    "    side_field: Optional[str]\n",
    "    side_mapping: Optional[Dict]\n",
    "    id_field: str\n",
    "    buy_no_field: Optional[str]\n",
    "    sell_no_field: Optional[str]\n",
    "\n",
    "\n",
    "EXCHANGE_REGISTRY = {\n",
    "    # 上海\n",
    "    'sh': {\n",
    "        \"order\": ExchangeDefinition(\n",
    "            symbol_field='SecurityID',\n",
    "            time_field=\"TickTime\",\n",
    "            event_field=\"TickType\",\n",
    "            event_mapping={\"A\": \"ADD\", \"D\": \"CANCEL\"},\n",
    "            price_field=\"Price\",\n",
    "            volume_field=\"Volume\",\n",
    "            side_field=\"Side\",\n",
    "            side_mapping={'1': \"B\", '2': \"S\"},\n",
    "            id_field=\"SubSeq\",\n",
    "            buy_no_field=None,\n",
    "            sell_no_field=None,\n",
    "        ),\n",
    "        \"trade\": ExchangeDefinition(\n",
    "            symbol_field='SecurityID',\n",
    "            time_field=\"TickTime\",\n",
    "            event_field=\"TickType\",\n",
    "            event_mapping={\"T\": \"TRADE\"},\n",
    "            price_field=\"Price\",\n",
    "            volume_field=\"Volume\",\n",
    "            side_field=\"Side\",\n",
    "            side_mapping={'1': \"B\", '2': \"S\"},\n",
    "            id_field=\"SubSeq\",\n",
    "            buy_no_field=\"BuyNo\",\n",
    "            sell_no_field=\"SellNo\",\n",
    "        ),\n",
    "    },\n",
    "\n",
    "    # 深圳\n",
    "    'sz': {\n",
    "        \"order\": ExchangeDefinition(\n",
    "            symbol_field='SecurityID',\n",
    "            time_field=\"OrderTime\",\n",
    "            event_field=\"OrderType\",\n",
    "            event_mapping={'0': \"CANCEL\", '1': \"ADD\", '2': \"ADD\", '3': \"ADD\"},\n",
    "            price_field=\"Price\",\n",
    "            volume_field=\"Volume\",\n",
    "            side_field=\"Side\",\n",
    "            side_mapping={'1': \"B\", '2': \"S\"},\n",
    "            id_field=\"SubSeq\",\n",
    "            buy_no_field=None,\n",
    "            sell_no_field=None,\n",
    "        ),\n",
    "\n",
    "        \"trade\": ExchangeDefinition(\n",
    "            symbol_field='SecurityID',\n",
    "            time_field=\"TickTime\",\n",
    "            event_field=\"ExecType\",\n",
    "            event_mapping={'1': \"TRADE\", '2': \"CANCEL\"},\n",
    "            price_field=\"TradePrice\",\n",
    "            volume_field=\"TradeVolume\",\n",
    "            side_field=None,\n",
    "            side_mapping=None,\n",
    "            id_field=\"SubSeq\",\n",
    "            buy_no_field=\"BuyNo\",\n",
    "            sell_no_field=\"SellNo\",\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# MAPPING_kind = {\n",
    "#     '1':'order',\n",
    "#     '2':'trade',\n",
    "# }\n",
    "\n",
    "# =============================================================================\n",
    "# 2. TickTime -> offset_us （执行层：Arrow vectorized）\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def trade_time_to_base_us(trade_time) -> int:\n",
    "    \"\"\"\n",
    "    使用 DateTimeUtils 作为唯一语义来源\n",
    "    \"\"\"\n",
    "    d = DateTimeUtils.extract_date(trade_time)\n",
    "\n",
    "    base_dt = datetime(\n",
    "        d.year,\n",
    "        d.month,\n",
    "        d.day,\n",
    "        tzinfo=DateTimeUtils.SH_TZ,\n",
    "    )\n",
    "    return int(base_dt.timestamp() * 1_000_000)\n",
    "\n",
    "\n",
    "def _mod(a: pa.Array, b: int) -> pa.Array:\n",
    "    \"\"\"\n",
    "    Arrow-safe modulo, version independent:\n",
    "        a % b == a - floor(a / b) * b\n",
    "    \"\"\"\n",
    "    return pc.subtract(\n",
    "        a,\n",
    "        pc.multiply(\n",
    "            pc.cast(pc.floor(pc.divide(a, b)), pa.int64()),\n",
    "            pa.scalar(b, pa.int64()),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def tick_to_offset_us(col: pa.Array) -> pa.Array:\n",
    "    t = pc.cast(col, pa.int64())\n",
    "\n",
    "    # HH\n",
    "    hh = pc.cast(pc.floor(pc.divide(t, 1_000_000)), pa.int64())\n",
    "\n",
    "    # MM\n",
    "    mm_all = pc.cast(pc.floor(pc.divide(t, 10_000)), pa.int64())\n",
    "    mm = _mod(mm_all, 100)\n",
    "\n",
    "    # SS\n",
    "    ss_all = pc.cast(pc.floor(pc.divide(t, 100)), pa.int64())\n",
    "    ss = _mod(ss_all, 100)\n",
    "\n",
    "    # mmm (milliseconds)\n",
    "    ms = _mod(t, 1_000)\n",
    "\n",
    "    return pc.add(\n",
    "        pc.add(\n",
    "            pc.add(\n",
    "                pc.multiply(hh, pa.scalar(3_600_000_000, pa.int64())),\n",
    "                pc.multiply(mm, pa.scalar(60_000_000, pa.int64())),\n",
    "            ),\n",
    "            pc.multiply(ss, pa.scalar(1_000_000, pa.int64())),\n",
    "        ),\n",
    "        pc.multiply(ms, pa.scalar(1_000, pa.int64())),\n",
    "    )\n",
    "\n",
    "\n",
    "def map_dict(col: pa.Array, mapping: dict) -> pa.Array:\n",
    "    keys = pa.array(list(mapping.keys()))\n",
    "    vals = pa.array(list(mapping.values()))\n",
    "    idx = pc.index_in(col, keys)\n",
    "    return pc.take(vals, idx)\n",
    "\n",
    "\n",
    "def zeros(n: int) -> pa.Array:\n",
    "    return pa.array([0] * n, type=pa.int64())\n",
    "\n",
    "\n",
    "def parse_events_arrow(\n",
    "        table: pa.Table,\n",
    "        kind: Literal[\"order\", \"trade\"] = '',\n",
    "        exchange: str = ''\n",
    ") -> pa.Table:\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        Arrow Table（单 symbol / 单 kind / 单 exchange）\n",
    "    输出：\n",
    "        Arrow Table（InternalEvent schema）\n",
    "    \"\"\"\n",
    "    if table.num_rows == 0:\n",
    "        return pa.Table.from_arrays([])\n",
    "\n",
    "    try:\n",
    "        definition = EXCHANGE_REGISTRY[exchange][kind]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"No registry for exchange={exchange}, kind={kind}\")\n",
    "    # # ---------------------------------------------------------------------\n",
    "    #     # ts\n",
    "    #     # ---------------------------------------------------------------------\n",
    "    # # print(table[\"TradeTime\"][0])\n",
    "    base_us = trade_time_to_base_us(table[\"TradeTime\"][0].as_py())\n",
    "    offset_us = tick_to_offset_us(table[definition.time_field])  # Array\n",
    "    ts = pc.add(offset_us, pa.scalar(base_us, pa.int64()))  # Array\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # event\n",
    "    # ---------------------------------------------------------------------\n",
    "    event = map_dict(table[definition.event_field], definition.event_mapping)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # side\n",
    "    # ---------------------------------------------------------------------\n",
    "    if definition.side_field and definition.side_mapping:\n",
    "        side = map_dict(table[definition.side_field], definition.side_mapping)\n",
    "    else:\n",
    "        side = pa.nulls(table.num_rows)\n",
    "    #\n",
    "    # ---------------------------------------------------------------------\n",
    "    # buy / sell no\n",
    "    # ---------------------------------------------------------------------\n",
    "    buy_no = (\n",
    "        table[definition.buy_no_field]\n",
    "        if definition.buy_no_field\n",
    "        else zeros(table.num_rows)\n",
    "    )\n",
    "    sell_no = (\n",
    "        table[definition.sell_no_field]\n",
    "        if definition.sell_no_field\n",
    "        else zeros(table.num_rows)\n",
    "    )\n",
    "    out = pa.table(\n",
    "        {\"symbol\": pc.cast(table[definition.symbol_field], pa.string()),\n",
    "         \"ts\": ts,\n",
    "         \"event\": event,\n",
    "         \"order_id\": pc.cast(table[definition.id_field], pa.int64()),\n",
    "         \"side\": side,\n",
    "         \"price\": pc.cast(table[definition.price_field], pa.float64()),\n",
    "         \"volume\": pc.cast(table[definition.volume_field], pa.int64()),\n",
    "         \"buy_no\": pc.cast(buy_no, pa.int64()),\n",
    "         \"sell_no\": pc.cast(sell_no, pa.int64()),\n",
    "         }\n",
    "    )\n",
    "\n",
    "    return out.cast(INTERNAL_SCHEMA)\n",
    "\n",
    "\n"
   ],
   "id": "e50a4d271c8b3123",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:42:56.596262981Z",
     "start_time": "2025-12-22T02:42:56.577968766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!filepath: src/engines/symbol_split_engine.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import pyarrow as pa\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class SymbolSplitEngine:\n",
    "    \"\"\"\n",
    "    SymbolSplitEngine（纯逻辑）：\n",
    "\n",
    "    Input:\n",
    "        - canonical Events.parquet（Arrow Table / Reader）\n",
    "        - symbol: str\n",
    "\n",
    "    Output:\n",
    "        - bytes（该 symbol 的 parquet 内容）\n",
    "\n",
    "    约束：\n",
    "        - 不做 IO\n",
    "        - 不依赖 Path\n",
    "        - 不接触 Meta\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, symbol_field: str = \"symbol\"):\n",
    "        self.symbol_field = symbol_field\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def split_one(\n",
    "            self,\n",
    "            table: pa.Table,\n",
    "            symbol: str,\n",
    "    ) -> bytes:\n",
    "        \"\"\"\n",
    "        从 canonical table 中切出某一个 symbol\n",
    "        \"\"\"\n",
    "        mask = pa.compute.equal(table[self.symbol_field], symbol)\n",
    "        sub = table.filter(mask)\n",
    "\n",
    "        sink = pa.BufferOutputStream()\n",
    "        pq.write_table(sub, sink)\n",
    "\n",
    "        return sink.getvalue().to_pybytes()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def split_many(\n",
    "            self,\n",
    "            table: pa.Table,\n",
    "            symbols: Iterable[str],\n",
    "    ) -> dict[str, bytes]:\n",
    "        \"\"\"\n",
    "        一次切多个 symbol（可选优化）\n",
    "        \"\"\"\n",
    "        result: dict[str, bytes] = {}\n",
    "\n",
    "        for sym in symbols:\n",
    "            result[sym] = self.split_one(table, sym)\n",
    "\n",
    "        return result\n"
   ],
   "id": "963820cef61d6132",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:47:47.905573110Z",
     "start_time": "2025-12-22T02:47:47.854211974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!filepath: src/steps/symbol_split_step.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from src.pipeline.step import PipelineStep\n",
    "from src.pipeline.meta import MetaRegistry\n",
    "\n",
    "\n",
    "class SymbolSplitStep(PipelineStep):\n",
    "    \"\"\"\n",
    "    SymbolSplitStep（Meta-aware，冻结版）\n",
    "\n",
    "    Semantic:\n",
    "        canonical Events.parquet\n",
    "            → symbol/{symbol}/{date}/Trade.parquet\n",
    "    SymbolSplitStep — DAILY-CLOSED (data-driven) FINAL VERSION\n",
    "\n",
    "    Semantics (FROZEN):\n",
    "\n",
    "    - Meta is DATE-scoped.\n",
    "    - Daily universe is defined ONLY by that day's meta.outputs.\n",
    "    - First run (no meta):\n",
    "        * Read canonical once\n",
    "        * Discover symbols appearing on THIS date\n",
    "        * Full split\n",
    "        * Write meta (universe = discovered symbols)\n",
    "    - Subsequent runs:\n",
    "        * Universe = meta.outputs.keys()\n",
    "        * If all outputs valid -> SKIP (NO canonical IO)\n",
    "        * If some outputs invalid/missing -> read canonical and repair ONLY those symbols\n",
    "    - Does NOT detect symbols missing due to upstream canonical issues.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            engine: SymbolSplitEngine,\n",
    "            inst=None,\n",
    "    ):\n",
    "        self.engine = engine\n",
    "        self.inst = inst\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def run(self, ctx):\n",
    "        input_dir: Path = ctx.canonical_dir\n",
    "        output_dir: Path = ctx.symbol_dir\n",
    "\n",
    "        meta_dir: Path = ctx.meta_dir\n",
    "\n",
    "        outputs = {}\n",
    "        for file in list(input_dir.glob(\"*.parquet\")):\n",
    "            # ① 修正 step 语义：pipeline step + file\n",
    "            step_key = f\"{self.__class__.__name__}:{file.stem}\"\n",
    "\n",
    "            meta = MetaRegistry(\n",
    "                meta_file=meta_dir / step_key,\n",
    "                step=file.stem,\n",
    "                date=ctx.date,\n",
    "                engine_version=\"v1\",\n",
    "                input_file=input_dir,\n",
    "            )\n",
    "            manifest = meta.load()\n",
    "\n",
    "            # ---------------------------------------------\n",
    "            # ① 决定需要 split 的 symbol\n",
    "            # ---------------------------------------------\n",
    "            if manifest is None or meta.is_input_changed():\n",
    "                table = pq.read_table(file, columns=[\"symbol\"])\n",
    "                symbols = table[\"symbol\"].unique().to_pylist()\n",
    "            else:\n",
    "                status = meta.validate_outputs()\n",
    "                symbols = [k for k, ok in status.items() if not ok]\n",
    "\n",
    "            if not symbols:\n",
    "                continue\n",
    "            # ② 读取 canonical table（一次）\n",
    "            table = pq.read_table(file)\n",
    "            # ③ 执行 split（纯逻辑）\n",
    "            payloads = self.engine.split_many(table, symbols)\n",
    "\n",
    "            # ④ 写文件 + 记录 meta\n",
    "            meta.begin_new()\n",
    "\n",
    "            for sym, data in payloads.items():\n",
    "                out_file = output_dir / sym / file.name.split('_')[1]\n",
    "                FileSystem.safe_write(out_file, data)\n",
    "                meta.record_output(sym, out_file)\n",
    "\n",
    "            meta.commit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "611bd539295a0180",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:47:48.861232570Z",
     "start_time": "2025-12-22T02:47:48.329455440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "cs = CsvConvertStep(engine=ConvertEngine(), inst=inst)\n",
    "cs.run(ctx)\n",
    "\n",
    "ns = NormalizeStep(engine=NormalizeEngine(), inst=inst)\n",
    "ns.run(ctx)\n",
    "\n",
    "sp = SymbolSplitStep(engine=SymbolSplitEngine(), inst=inst)\n",
    "sp.run(ctx)\n",
    "\n"
   ],
   "id": "3ab1329eeef0d695",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CsvConvertStep]  SZ_Order.csv.7z {'sz_order': PosixPath('/home/wsw/data/parquet/2015-01-01/sz_order.parquet')}\n",
      "[CsvConvertStep]  SH_Stock_OrderTrade.csv.7z {'sh_order': PosixPath('/home/wsw/data/parquet/2015-01-01/sh_order.parquet'), 'sh_trade': PosixPath('/home/wsw/data/parquet/2015-01-01/sh_trade.parquet')}\n",
      "[CsvConvertStep]  SZ_Trade.csv.7z {'sz_trade': PosixPath('/home/wsw/data/parquet/2015-01-01/sz_trade.parquet')}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file '/home/wsw/data/canonical/2015-01-01/sh_order.parquet'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m cs.run(ctx)\n\u001B[32m      4\u001B[39m ns = NormalizeStep(engine=NormalizeEngine(), inst=inst)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[43mns\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m sp = SymbolSplitStep(engine=SymbolSplitEngine(), inst=inst)\n\u001B[32m      8\u001B[39m sp.run(ctx)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 80\u001B[39m, in \u001B[36mNormalizeStep.run\u001B[39m\u001B[34m(self, ctx)\u001B[39m\n\u001B[32m     78\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_file.exists():\n\u001B[32m     79\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m80\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     81\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_dir\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     83\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 39\u001B[39m, in \u001B[36mNormalizeEngine.execute\u001B[39m\u001B[34m(self, input_file, output_dir)\u001B[39m\n\u001B[32m     37\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m     38\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m         writer = \u001B[43mpq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mParquetWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtable\u001B[49m\u001B[43m.\u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m     writer.write_table(table)\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m writer:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/envs/dev/lib/python3.13/site-packages/pyarrow/parquet/core.py:1064\u001B[39m, in \u001B[36mParquetWriter.__init__\u001B[39m\u001B[34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, store_decimal_as_integer, **options)\u001B[39m\n\u001B[32m   1059\u001B[39m filesystem, path = _resolve_filesystem_and_path(where, filesystem)\n\u001B[32m   1060\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m filesystem \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1061\u001B[39m     \u001B[38;5;66;03m# ARROW-10480: do not auto-detect compression.  While\u001B[39;00m\n\u001B[32m   1062\u001B[39m     \u001B[38;5;66;03m# a filename like foo.parquet.gz is nonconforming, it\u001B[39;00m\n\u001B[32m   1063\u001B[39m     \u001B[38;5;66;03m# shouldn't implicitly apply compression.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1064\u001B[39m     sink = \u001B[38;5;28mself\u001B[39m.file_handle = \u001B[43mfilesystem\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen_output_stream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   1066\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1067\u001B[39m     sink = where\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/envs/dev/lib/python3.13/site-packages/pyarrow/_fs.pyx:913\u001B[39m, in \u001B[36mpyarrow._fs.FileSystem.open_output_stream\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/envs/dev/lib/python3.13/site-packages/pyarrow/error.pxi:155\u001B[39m, in \u001B[36mpyarrow.lib.pyarrow_internal_check_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/envs/dev/lib/python3.13/site-packages/pyarrow/error.pxi:92\u001B[39m, in \u001B[36mpyarrow.lib.check_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] Failed to open local file '/home/wsw/data/canonical/2015-01-01/sh_order.parquet'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:40:50.110839061Z",
     "start_time": "2025-12-22T02:40:50.094156970Z"
    }
   },
   "cell_type": "code",
   "source": "t = pq.read_table('/home/wsw/data/canonical/2015-01-01/sh_order.parquet')",
   "id": "dc7eade650dfaa78",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.shape",
   "id": "54797dc1e707845",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.slice(0, 5)",
   "id": "83684fd71b574fd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "t.take([0, 1, 2])",
   "id": "47f8cbdcb61b9e49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import json",
   "id": "3ffa04a896710f9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('/home/wsw/data/meta/2015-01-01/SymbolSplitStep:sh_order.parquet.json') as f:\n",
    "    details = json.load(f)"
   ],
   "id": "960dba2a78c49dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "details",
   "id": "ab78ebecdb0ca041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('/home/wsw/data/meta/2015-01-01/SymbolSplitStep:sh_trade.parquet.json') as f:\n",
    "    details2 = json.load(f)\n",
    "details2"
   ],
   "id": "d0cfa57580c6b3ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3430460bc9db9f23",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
