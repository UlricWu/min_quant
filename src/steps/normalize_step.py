# src/pipeline/steps/normalize_step.py
from __future__ import annotations

from pathlib import Path

from src import logs
from src.pipeline.context import PipelineContext
from src.pipeline.step import PipelineStep
from src.pipeline.parallel.executor import ParallelExecutor
from src.pipeline.parallel.types import ParallelKind
from src.meta.meta import BaseMeta
from src.engines.normalize_engine import NormalizeEngine


def normalize_one(
        *,
        input_file: Path,
        output_dir: Path,
) -> dict:
    """
    Normalize workerï¼ˆè¿›ç¨‹å®‰å…¨ï¼‰

    çº¦æŸï¼š
    - top-level function
    - ä¸ä¾èµ– ctx / self
    - ä¸å†™ meta
    - ä¸å†™æ—¥å¿—
    """
    engine = NormalizeEngine()
    return engine.execute(
        input_file=input_file,
        output_dir=output_dir,
    )


class NormalizeStep(PipelineStep):
    """
    NormalizeStepï¼ˆProcessPool å¹¶è¡Œç‰ˆï¼‰

    å¹¶è¡Œæ¨¡åž‹ï¼š
        - ä¸»è¿›ç¨‹ï¼šmeta åˆ¤å®š + commit
        - å­è¿›ç¨‹ï¼šçº¯ normalize
    """

    def __init__(
            self,
            inst=None,
            max_workers: int | None = 2,
    ):
        super().__init__(inst)
        self.max_workers = max_workers

    def run(self, ctx: PipelineContext) -> PipelineContext:
        meta = BaseMeta(ctx.meta_dir, stage="normalize")

        # --------------------------------------------------
        # â‘  masterï¼šç­›é€‰éœ€è¦å¤„ç†çš„æ–‡ä»¶
        # --------------------------------------------------
        inputs: list[Path] = []
        for input_file in ctx.parquet_dir.glob("*.parquet"):
            if not meta.upstream_changed(input_file):
                logs.info(
                    f"[NormalizeStep] {input_file.name} unchanged -> skip"
                )
                continue
            inputs.append(input_file)

        if not inputs:
            logs.info("[NormalizeStep] no files to normalize")
            return ctx

        # --------------------------------------------------
        # â‘¡ masterï¼šå‡†å¤‡å¹¶è¡Œå‚æ•°ï¼ˆçº¯æ•°æ®ï¼‰
        # --------------------------------------------------
        items = [
            {
                "input_file": path,
                "output_dir": ctx.canonical_dir,
            }
            for path in inputs
        ]

        # --------------------------------------------------
        # â‘¢ å¹¶è¡Œæ‰§è¡Œï¼ˆæ— é—­åŒ…ï¼‰
        # --------------------------------------------------
        with self.inst.timer(
                f"[NormalizeStep] parallel normalize | files={len(items)}"
        ):

            results = ParallelExecutor.run(
                kind=ParallelKind.FILE,
                items=items,
                handler=_normalize_handler,
                max_workers=self.max_workers,
            )

        # --------------------------------------------------
        # â‘£ masterï¼šç»Ÿä¸€ commit meta
        # --------------------------------------------------
        outputs: list[str] = []
        for result in results or []:
            meta.commit(result)
            name = result.output_file.stem  # e.g. "sz_trade"
            outputs.append(name)
        if results is None:
            raise RuntimeError(
                "[NormalizeStep] ParallelExecutor returned None"
            )

        logs.info(f'[NormalizeStep] files {outputs}  to normalize')

        # ðŸ”’ å”¯ä¸€å¯ä¿¡è¾“å…¥é›†
        ctx.normalize_outputs = outputs

        return ctx


def _normalize_handler(payload: dict) -> dict:
    """
    ProcessPool handlerï¼ˆtop-levelï¼‰
    """
    return normalize_one(
        input_file=payload["input_file"],
        output_dir=payload["output_dir"],
    )
